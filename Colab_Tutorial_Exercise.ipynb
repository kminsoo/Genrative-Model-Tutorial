{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/MyDrive/Genrative-Model-Tutorial\n","!pip install -r requirements.txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P7iHW6GFXgEN","executionInfo":{"status":"ok","timestamp":1694958135497,"user_tz":-540,"elapsed":49718,"user":{"displayName":"Minsoo Kang","userId":"07508484871301357904"}},"outputId":"e4ba4d68-a80a-47e2-f164-78984740ed9f"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/Genrative-Model-Tutorial\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (1.4.0)\n","Collecting blessings (from -r requirements.txt (line 2))\n","  Downloading blessings-1.7-py3-none-any.whl (18 kB)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (2023.7.22)\n","Collecting dominate (from -r requirements.txt (line 4))\n","  Downloading dominate-2.8.0-py2.py3-none-any.whl (29 kB)\n","Requirement already satisfied: grpcio in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (1.57.0)\n","Requirement already satisfied: Markdown in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (3.4.4)\n","Collecting nvidia-ml-py3 (from -r requirements.txt (line 7))\n","  Downloading nvidia-ml-py3-7.352.0.tar.gz (19 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting olefile (from -r requirements.txt (line 8))\n","  Downloading olefile-0.46.zip (112 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.2/112.2 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (4.8.0.76)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (9.4.0)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 11)) (3.20.3)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 12)) (5.9.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 13)) (1.11.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 14)) (1.16.0)\n","Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 15)) (2.13.0)\n","Collecting tensorboardX (from -r requirements.txt (line 16))\n","  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torchprofile (from -r requirements.txt (line 17))\n","  Downloading torchprofile-0.0.4-py3-none-any.whl (7.7 kB)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 18)) (4.66.1)\n","Requirement already satisfied: Werkzeug in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 19)) (2.3.7)\n","Collecting wget (from -r requirements.txt (line 20))\n","  Downloading wget-3.2.zip (10 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting wandb (from -r requirements.txt (line 21))\n","  Downloading wandb-0.15.10-py3-none-any.whl (2.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 22)) (4.6.6)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 23)) (3.7.1)\n","Requirement already satisfied: ipykernel in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 24)) (5.5.6)\n","Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python-headless->-r requirements.txt (line 9)) (1.23.5)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 15)) (2.17.3)\n","Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 15)) (1.0.0)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 15)) (2.31.0)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 15)) (67.7.2)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 15)) (0.7.1)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 15)) (0.41.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboardX->-r requirements.txt (line 16)) (23.1)\n","Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.10/dist-packages (from torchprofile->-r requirements.txt (line 17)) (2.0.1+cu118)\n","Requirement already satisfied: torchvision>=0.4 in /usr/local/lib/python3.10/dist-packages (from torchprofile->-r requirements.txt (line 17)) (0.15.2+cu118)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from Werkzeug->-r requirements.txt (line 19)) (2.1.3)\n","Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 21)) (8.1.7)\n","Collecting GitPython!=3.1.29,>=1.0.0 (from wandb->-r requirements.txt (line 21))\n","  Downloading GitPython-3.1.36-py3-none-any.whl (189 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.5/189.5 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting sentry-sdk>=1.0.0 (from wandb->-r requirements.txt (line 21))\n","  Downloading sentry_sdk-1.31.0-py2.py3-none-any.whl (224 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.8/224.8 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb->-r requirements.txt (line 21))\n","  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 21)) (6.0.1)\n","Collecting pathtools (from wandb->-r requirements.txt (line 21))\n","  Downloading pathtools-0.1.2.tar.gz (11 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting setproctitle (from wandb->-r requirements.txt (line 21))\n","  Downloading setproctitle-1.3.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n","Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 21)) (1.4.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown->-r requirements.txt (line 22)) (3.12.2)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown->-r requirements.txt (line 22)) (4.11.2)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 23)) (1.1.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 23)) (0.11.0)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 23)) (4.42.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 23)) (1.4.5)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 23)) (3.1.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 23)) (2.8.2)\n","Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.10/dist-packages (from ipykernel->-r requirements.txt (line 24)) (0.2.0)\n","Requirement already satisfied: ipython>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from ipykernel->-r requirements.txt (line 24)) (7.34.0)\n","Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from ipykernel->-r requirements.txt (line 24)) (5.7.1)\n","Requirement already satisfied: jupyter-client in /usr/local/lib/python3.10/dist-packages (from ipykernel->-r requirements.txt (line 24)) (6.1.12)\n","Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipykernel->-r requirements.txt (line 24)) (6.3.2)\n","Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 21))\n","  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 15)) (5.3.1)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 15)) (0.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 15)) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard->-r requirements.txt (line 15)) (1.3.1)\n","Collecting jedi>=0.16 (from ipython>=5.0.0->ipykernel->-r requirements.txt (line 24))\n","  Downloading jedi-0.19.0-py2.py3-none-any.whl (1.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m74.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->-r requirements.txt (line 24)) (4.4.2)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->-r requirements.txt (line 24)) (0.7.5)\n","Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->-r requirements.txt (line 24)) (3.0.39)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->-r requirements.txt (line 24)) (2.16.1)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->-r requirements.txt (line 24)) (0.2.0)\n","Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->-r requirements.txt (line 24)) (0.1.6)\n","Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel->-r requirements.txt (line 24)) (4.8.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 15)) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 15)) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 15)) (2.0.4)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->torchprofile->-r requirements.txt (line 17)) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->torchprofile->-r requirements.txt (line 17)) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->torchprofile->-r requirements.txt (line 17)) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->torchprofile->-r requirements.txt (line 17)) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->torchprofile->-r requirements.txt (line 17)) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.4->torchprofile->-r requirements.txt (line 17)) (3.27.4.1)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.4->torchprofile->-r requirements.txt (line 17)) (16.0.6)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown->-r requirements.txt (line 22)) (2.5)\n","Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-client->ipykernel->-r requirements.txt (line 24)) (5.3.1)\n","Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.10/dist-packages (from jupyter-client->ipykernel->-r requirements.txt (line 24)) (23.2.1)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 15)) (1.7.1)\n","Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 21))\n","  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n","Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=5.0.0->ipykernel->-r requirements.txt (line 24)) (0.8.3)\n","Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.6.0->jupyter-client->ipykernel->-r requirements.txt (line 24)) (3.10.0)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=5.0.0->ipykernel->-r requirements.txt (line 24)) (0.7.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5.0.0->ipykernel->-r requirements.txt (line 24)) (0.2.6)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 15)) (0.5.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard->-r requirements.txt (line 15)) (3.2.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.4->torchprofile->-r requirements.txt (line 17)) (1.3.0)\n","Building wheels for collected packages: nvidia-ml-py3, olefile, wget, pathtools\n","  Building wheel for nvidia-ml-py3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for nvidia-ml-py3: filename=nvidia_ml_py3-7.352.0-py3-none-any.whl size=19171 sha256=f4f2c129e20878426f53712c8a352e449ef77df766db9e1a7a36dc068c217f7f\n","  Stored in directory: /root/.cache/pip/wheels/5c/d8/c0/46899f8be7a75a2ffd197a23c8797700ea858b9b34819fbf9e\n","  Building wheel for olefile (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for olefile: filename=olefile-0.46-py2.py3-none-any.whl size=35417 sha256=5fe80cf35e589a1777316445e3ab969b1e7df919019d334df73509c8fe23e817\n","  Stored in directory: /root/.cache/pip/wheels/02/39/c0/9eb1f7a42b4b38f6f333b6314d4ed11c46f12a0f7b78194f0d\n","  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9655 sha256=9049ea5325e09e64711c3d3c98448fba32dc39aef32cd3145fc9040bfb1d06c6\n","  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n","  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=51d5f4064035827950ddb0d0753c7177612f2d19261b268a2916082cd2171afb\n","  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n","Successfully built nvidia-ml-py3 olefile wget pathtools\n","Installing collected packages: wget, pathtools, nvidia-ml-py3, tensorboardX, smmap, setproctitle, sentry-sdk, olefile, jedi, dominate, docker-pycreds, blessings, gitdb, GitPython, wandb, torchprofile\n","Successfully installed GitPython-3.1.36 blessings-1.7 docker-pycreds-0.4.0 dominate-2.8.0 gitdb-4.0.10 jedi-0.19.0 nvidia-ml-py3-7.352.0 olefile-0.46 pathtools-0.1.2 sentry-sdk-1.31.0 setproctitle-1.3.2 smmap-5.0.1 tensorboardX-2.6.2.2 torchprofile-0.0.4 wandb-0.15.10 wget-3.2\n"]}]},{"cell_type":"markdown","source":["# Google Drive Mount and Install"],"metadata":{"id":"ayNGlR6PXhtA"}},{"cell_type":"markdown","metadata":{"id":"oOS7Of0-W6aQ"},"source":["# VEM combined with OMGD on Horse2Zebra using CycleGAN\n","\n","\n",">## OMGD\n","\n",">* OMGD is an online distillation framework which jointly trains a teacher and a student.\n","\n",">## Teacher Training using CycleGAN objective\n","![](https://drive.google.com/uc?id=1nPPyRjH_PCncmSMa7Avh8wlfKaHWEy6N)\n","\n","> * The generator is trained to fool the discriminator, it wants to output data that looks _as close as possible_ to real, training data.\n","* The discriminator is a classifier that is trained to figure out which data is real and which is fake.\n","\n",">## Student and Energy-based Model Training\n","![](https://drive.google.com/uc?id=1uWeSe4AmaAU4xBSN9G_d02PmzMLPJrkZ)\n","\n","> * EBM is optimized to minimize the difference between the true mutual information and its lower bound esimated by EBM.\n","\n","> * Student is trained to minimize the intermediate feature map and output distillation losses while maximizing the mutual information between the teacher and the studnet. In addition, we learn the student to minimize the total variation loss for spatial smoothness.\n","\n","> * In this lecture, we do not implement the inintermediate feature map distillation loss, so if you want to reproduce the algorithm, please run mi_distill.sh in `./scripts/cycle_gan/horse2zebra`\n","\n","\n"]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"PMZnBZ6EW6aS"},"source":["# 0. Setup for downloading horse2zebra dataset"]},{"cell_type":"markdown","metadata":{"id":"23n5OlRmW6aT"},"source":["* Download horse-zebra dataset\n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"7_uAecO6W6aT"},"outputs":[],"source":["import gdown\n","import os\n","file_id = \"1wMiqnCAPlXvhmpvT5x65T007fFrjRW1D\"\n","os.makedirs('./database', exist_ok=True)\n","gdown.download(id=file_id,output='./database/horse2zebra.zip', quiet=False)\n","os.system(\"unzip ./database/horse2zebra.zip -d ./database\")"]},{"cell_type":"markdown","metadata":{"id":"AigRcEt-W6aU"},"source":["# 1. Construct the unaligned horse2zebra dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"TIHYGhXhW6aU"},"outputs":[],"source":["import torch.utils.data as data\n","import os\n","import torchvision.transforms as transforms\n","from PIL import Image\n","import random\n","\n","IMG_EXTENSIONS = [\n","    '.jpg', '.JPG', '.jpeg', '.JPEG',\n","    '.png', '.PNG', '.ppm', '.PPM', '.bmp', '.BMP',\n","]\n","def make_dataset(dir, max_dataset_size=float('inf')):\n","    assert os.path.isdir(dir) or os.path.islink(dir), '%s is not a valid directory' % dir\n","    images = []\n","    for root, dnames, fnames in sorted(os.walk(dir)):\n","        for fname in fnames:\n","            if any(fname.endswith(extension) for extension in IMG_EXTENSIONS):\n","                path = os.path.join(root, fname)\n","                images.append(path)\n","    return images[:min(max_dataset_size, len(images))]\n","\n","class UnalignedDataset(data.Dataset):\n","    \"\"\"\n","    This dataset class can load unaligned/unpaired datasets.\n","\n","    It requires two directories to host training images from domain A '/path/to/data/trainA'\n","    and from domain B '/path/to/data/trainB' respectively.\n","    You can train the model with the dataset flag '--dataroot /path/to/data'.\n","    Similarly, you need to prepare two directories:\n","    '/path/to/data/testA' and '/path/to/data/testB' during test time.\n","    \"\"\"\n","    def __init__(self, dataroot, phase='train', load_size=286, size=256):\n","        self.dir_A = os.path.join(dataroot, phase + 'A') # \"./daabase/horse2zebra/trainA\"\n","        self.dir_B = os.path.join(dataroot, phase + 'B') # \"./daabase/horse2zebra/trainB\"\n","        self.A_paths = sorted(make_dataset(self.dir_A))\n","        # self.A_paths: List[str] = ['./daabase/horse2zebra/trainA/name1.jpg', './daabase/horse2zebra/trainA/name2.jpg', ..., './daabase/horse2zebra/trainA/nameN.jpg']\n","        self.B_paths = sorted(make_dataset(self.dir_B))\n","        # self.B_paths: List[str] = ['./daabase/horse2zebra/trainB/name1.jpg', './daabase/horse2zebra/trainB/name2.jpg', ..., './daabase/horse2zebra/trainB/nameN.jpg']\n","        self.A_size = len(self.A_paths)\n","        self.B_size = len(self.B_paths)\n","        self.transform_A = self._get_transform(load_size, size)\n","        self.transform_B = self._get_transform(load_size, size)\n","\n","\n","    def _get_transform(self, load_size=286, size=256):\n","        \"\"\"\n","        Resize an input image as load_size x load_size and then crop the resized image to size x size\n","        \"\"\"\n","        transform_list = []\n","        resize = [load_size, load_size]\n","        transform_list.append(transforms.Resize(resize, Image.BICUBIC)) # Resize the input image as load_size x load_size\n","        transform_list.append(transforms.RandomCrop(size)) # Randomly crop the resized image to size x size\n","        transform_list.append(transforms.ToTensor()) # Make the cropped image whose range is in [0,1]\n","        transform_list.append(transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))) # Normalize the image in [-1, 1]\n","\n","        return transforms.Compose(transform_list)\n","\n","    def __getitem__(self, idx_A):\n","        \"\"\"Return a data point and its metadata information.\n","\n","        Parameters:\n","            index (int)      -- a random integer for data indexing\n","\n","        Returns a dictionary that contains A, B, A_path and B_path\n","            A (tensor)       -- an image in the input domain\n","            B (tensor)       -- an image in the target domain\n","            A_path (str)    -- image path for domain A\n","            B_path (str)    -- image path for domain B\n","        \"\"\"\n","\n","        A_path = self.A_paths[idx_A % self.A_size] # make sure index is within then range\n","        idx_B = random.randint(0, self.B_size - 1) # randomize the index for domain B to avoid fixed pairs.\n","        B_path = self.B_paths[idx_B]\n","        A_img = Image.open(A_path).convert('RGB') # Read image using A_path\n","        B_img = Image.open(B_path).convert('RGB') # Read image using B_path\n","        # apply image transformation\n","        A = self.transform_A(A_img) # use transform_A\n","        B = self.transform_B(B_img) # use transform_B\n","        return {'A': A, 'B': B, 'A_paths': A_path, 'B_paths': B_path}\n","\n","    def __len__(self):\n","        # retunr the maximum size of dataset A and B\n","        return max(self.A_size, self.B_size)"]},{"cell_type":"markdown","metadata":{"id":"JtiK6-bXW6aV"},"source":["## Visualize the data\n","We define an inverse transform that changes a tensor back to a PIL Image. First, we \"unnormalize\" the tensor by taking the inverse of the normalization function from above. Then, we can call ```transforms.ToPILImage()``` which will convert the tensor to a PIL Image"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"1XneewTgW6aV"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","# Some matplotlib settings\n","plt.rcParams[\"figure.figsize\"] = (16, 10)\n","plt.rcParams[\"axes.grid\"] = False\n","plt.rcParams[\"xtick.major.bottom\"] = False\n","plt.rcParams[\"ytick.major.left\"] = False\n","def get_pil_img_from_tensor(img_tensor):\n","    # incerse_transform: make tensor whose range in [-1,1] -> PIL Image whose range in uint8 ([0,255])\n","    inverse_transform = transforms.Compose([transforms.Normalize(mean=[-1, -1, -1], std=[1/0.5, 1/0.5, 1/0.5]),\n","                                            transforms.ToPILImage()])\n","    return inverse_transform(img_tensor)\n","\n","def show_img(pil_img):\n","    plt.imshow(pil_img)"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"mirYolpPW6aV"},"outputs":[],"source":["import torch, torchvision\n","dataroot = './database/horse2zebra'\n","horse2zebra_dataset = UnalignedDataset(dataroot= dataroot)\n","visualization_horse_data = torch.cat([horse2zebra_dataset[i]['A'].unsqueeze(0) for i in range(16)], 0)\n","show_img(get_pil_img_from_tensor(torchvision.utils.make_grid(visualization_horse_data)))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"Ix9gDbdJW6aV"},"outputs":[],"source":["dataroot = './database/horse2zebra'\n","visualization_zebra_data = torch.cat([horse2zebra_dataset[i]['B'].unsqueeze(0) for i in range(16)], 0)\n","show_img(get_pil_img_from_tensor(torchvision.utils.make_grid(visualization_zebra_data)))\n"]},{"cell_type":"markdown","metadata":{"id":"eCEZBnueW6aW"},"source":["## Define Dataset and generate dataloaders"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"ue8mabhbW6aW"},"outputs":[],"source":["from torch.utils.data import DataLoader\n","# create DataLoader\n","def create_dataloader(dataroot='./database/horse2zebra', shuffle=True, batch_size=1, num_workers=4):\n","    horse2zebra_dataset = UnalignedDataset(dataroot= dataroot)\n","    dataloader = DataLoader(dataset=horse2zebra_dataset,\n","                              batch_size=batch_size,\n","                              shuffle=shuffle,\n","                              num_workers=num_workers)\n","    return dataloader\n","\n",""]},{"cell_type":"markdown","metadata":{"id":"wPKMK-aHW6aW"},"source":["# 2. Define the model\n","## Component:\n","\n","> ### Two discriminators (A->B and B->A)\n","> ### Two teacher generators (A->B and B->A)\n","> ### One student generator (A->B)\n","> ### One enrgy-based model for the student"]},{"cell_type":"markdown","metadata":{"id":"Xl8qkLt0W6aW"},"source":["## Define discriminator for training a teacher generator (Norm: nn.InstanceNorm2d)\n","![](https://drive.google.com/uc?id=12QlnJbLExv_DpeYIOsAUnd0rqL6T7qho)"]},{"cell_type":"markdown","metadata":{"id":"5lBOlZpCW6aW"},"source":["### Let's create the Discriminator!\n","\n","> **Exercise**: Create a generator model using nn.Conv2d(padding_mode='zeros'), nn.LeakyReLU, and nn.InstanceNorm2d function. Refer to the image above for what the network should look like!\n","\n","\n","> Refer to nn.Conv2d output size formula:\n","```\n","output_size = 1 + (input_size - kernel_size + 2*padding) / stride\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"baVrgyD4W6aW"},"outputs":[],"source":["import torch.nn as nn\n","class Discriminator(nn.Module):\n","    \"\"\"Defines a PatchGAN discriminator\"\"\"\n","\n","    def __init__(self, input_nc=3, ndf=128, n_layers=3, norm_layer=nn.InstanceNorm2d):\n","        \"\"\"Construct a PatchGAN discriminator\n","\n","        Parameters:\n","            input_nc (int)  -- the number of channels in input images\n","            ndf (int)       -- the number of filters in the last conv layer\n","            n_layers (int)  -- the number of conv layers in the discriminator\n","            norm_layer      -- normalization layer\n","        \"\"\"\n","        super(Discriminator, self).__init__()\n","        # Define self.model\n","        ??????\n","\n","    def forward(self, input):\n","\n","        return self.model(input)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"LIiCbecLW6aW"},"outputs":[],"source":["import torch\n","\"\"\"\n","Test Code: Check your implementation\n","\"\"\"\n","\n","with torch.no_grad():\n","    Test_D = Discriminator(input_nc=3,)\n","    input_ = torch.randn(1,3,256,256)\n","    output = Test_D(input_)\n","    print (output.size())\n","    assert output.size() == (1,1,30,30), \"check the network output shape\""]},{"cell_type":"markdown","metadata":{"id":"Gb-vIxe9W6aX"},"source":["## Define generator based on a residual structure (Norm: nn.InstanceNorm2d)  \n","![](https://drive.google.com/uc?id=1xqHoVF3NscajvJzTuSXSMm8veuWpjcp9)"]},{"cell_type":"markdown","metadata":{"id":"qn5LamhgW6aX"},"source":["###  Define the MobileResidualBlock (Norm: nn.InstanceNorm2d)\n","![](https://drive.google.com/uc?id=17mI_kDhJuN0sZ0qDh2PPaU7McPzsOWbB)"]},{"cell_type":"markdown","metadata":{"id":"oVOkU96lW6aX"},"source":["### Let's create the Generator!\n","\n","> **Exercise**: Create a generator model using nn.Conv2d(padding_mode='reflect'), nn.ConvTranspose2d, nn.ReLU, and nn.InstanceNorm2d function. Refer to the image above for what the network should look like!\n","\n","> nn.Conv2d output size formula:\n","```\n","output_size = 1 + (input_size - kernel_size + 2*padding) // stride\n","```\n","\n",">nn.ConvTranspose2d size formula\n","```\n","output_size = 1 + (input_size − 1) × stride − 2×padding + (kernel_size - 1) + output_padding\n","```\n","\n",">In case of depthwise cconvolution, you can employ nn.Conv2d(groups=in_channels), where 'in_channels' denotes the number of the input feature maps."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"JODvD8aWW6aX"},"outputs":[],"source":["import torch.nn as nn\n","class SeparableConv2d(nn.Module):\n","    def __init__(self, in_channels, out_channels, kernel_size, padding, norm_layer, stride=1, scale_factor=1):\n","        super(SeparableConv2d, self).__init__()\n","        # Define\n","        ?????  self.conv conists of 3x3 depthwise conv, norm and 1x1 conv\n","        self.conv =\n","\n","    def forward(self, x):\n","        return self.conv(x)\n","\n","class MobileResnetBlock(nn.Module):\n","    def __init__(self, dim, norm_layer):\n","        super(MobileResnetBlock, self).__init__()\n","\n","        conv_block = [\n","            SeparableConv2d(????),\n","            norm????, ReLU???\n","        ]\n","\n","        conv_block += [\n","            SeparableConv2d(????)\n","             norm???\n","        ]\n","\n","        self.conv_block = nn.Sequential(*conv_block)\n","\n","    def forward(self, x):\n","        out = x + self.conv_block(x)\n","        return out\n","\n","class Generator(nn.Module):\n","    def __init__(self, input_nc, output_nc, ngf, norm_layer=nn.InstanceNorm2d, n_blocks=9):\n","        super(Generator, self).__init__()\n","        #Define\n","        #Note that self.model which has n_blocks for MobileResnetBLock\n","\n","    def forward(self, input):\n","        return self.model(input)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"mICJ9lp3W6aX"},"outputs":[],"source":["import torch\n","\"\"\"\n","Test Code: Check your implementation\n","\"\"\"\n","with torch.no_grad():\n","    Test_G = Generator(input_nc=3, output_nc=3, ngf=64)\n","    input_ = torch.randn(1,3,256,256)\n","    output = Test_G(input_)\n","    print (output.size())\n","    assert output.size() == (1,3,256,256), \"check the network output shape\""]},{"cell_type":"markdown","metadata":{"id":"xgxpeg5yW6aX"},"source":["## Define energy-based model\n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"zsmfiPkNW6aY"},"outputs":[],"source":["import torch\n","from util import ResnetEBM\n","with torch.no_grad():\n","    Test_ebm = ResnetEBM(8, 7)\n","    t_input = torch.randn([1,3,256,256])\n","    s_input = torch.randn([1,3,256,256])\n","    output = Test_ebm(t_input, s_input)\n","    print (output.size())\n","    assert output.size() == (1,1), \"check the network output shape\""]},{"cell_type":"markdown","metadata":{"id":"tqP6T8l8W6aY"},"source":["# Model Creator"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"vN-JQZf3W6aY"},"outputs":[],"source":["def create_model(Teacher_G_ngf=64, Student_G_ngf=16, Teacher_D_ndf=128, ebm_input_nc=8, device='cuda'):\n","\n","    # Instantiate Teacher (A->B & B->A) and Student (A->B)) generators\n","    netG_teacher_A = Generator(input_nc=3, output_nc=3, ngf=Teacher_G_ngf, norm_layer=nn.InstanceNorm2d, n_blocks=9) # (A->B)\n","    netG_teacher_B = Generator(input_nc=3, output_nc=3, ngf=Teacher_G_ngf, norm_layer=nn.InstanceNorm2d, n_blocks=9) # (B->A)\n","    netG_student = Generator(input_nc=3, output_nc=3, ngf=Student_G_ngf, norm_layer=nn.InstanceNorm2d, n_blocks=9) # (A->B)\n","\n","    # Instantiate Discriminators for training Teacher Generators\n","    netD_teacher_A = Discriminator(input_nc=3, ndf=Teacher_D_ndf) # netD_teacher_A v.s. netG_teacher_A\n","    netD_teacher_B = Discriminator(input_nc=3, ndf=Teacher_D_ndf) # netD_teacher_B v.s. netG_teacher_B\n","\n","\n","    # Instantiate Energy-based Model for mutual information maximization\n","    netEBM = ResnetEBM(nec=ebm_input_nc, n_blocks=7) #\n","\n","    # Cast to appropriate device.\n","    netG_teacher_A.to(device)\n","    netG_teacher_A.train()\n","    netG_teacher_B.to(device)\n","    netG_teacher_B.train()\n","    netG_student.to(device)\n","    netG_student.train()\n","    netD_teacher_A.to(device)\n","    netD_teacher_A.train()\n","    netD_teacher_B.to(device)\n","    netD_teacher_B.train()\n","    netEBM.to(device)\n","    netEBM.train()\n","    print('Models loaded on {}'.format(device))\n","\n","    return netG_teacher_A, netG_teacher_B, netG_student, netD_teacher_A, netD_teacher_B, netEBM\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"Sq4M3GKuW6aY"},"outputs":[],"source":["# Device Settings\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","netG_teacher_A, netG_teacher_B, netG_student, netD_teacher_A, netD_teacher_B, netEBM = \\\n","create_model()"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"Bcc3P5YSW6aY"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"CL3u4NyPW6aY"},"source":["# 3. Define the loss functions"]},{"cell_type":"markdown","metadata":{"id":"s6GtWa2kW6aY"},"source":["## 3.1 Teacher Objective\n","![](https://drive.google.com/uc?id=1nPPyRjH_PCncmSMa7Avh8wlfKaHWEy6N)\n","\n","> * The generator is trained to fool the discriminator, it wants to output data that looks _as close as possible_ to real, training data.\n","* The discriminator is a classifier that is trained to figure out which data is real and which is fake."]},{"cell_type":"markdown","metadata":{"id":"i5aHMA9dW6aY"},"source":["We will use LSGAN loss for training teacher discriminators instead of the logistic loss.\n","\n","Let's design the discriminator loss using the following LSGAN loss equation\n","![](https://drive.google.com/uc?id=1bBE564KBTHnMJG13dwplz-U5Cv2UJhVy)\n","\n","But, our discriminator output is not a scalar value. To address the issue, we naively apply the LSGAN loss to the discriminator output at each pixel and then average them over all pixels."]},{"cell_type":"markdown","metadata":{"id":"8y02XEl8W6aY"},"source":["### Let's design the LSGAN loss!\n","\n","> **Exercise**: Create a LSGAN loss using the above equation."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"chYheTdHW6aZ"},"outputs":[],"source":["def discriminator_lsgan_loss(D_out_real, D_out_fake):\n","    # Define loss_for_real and loss_for_fake\n","    loss_for_real = ????\n","    loss_for_fake = ????\n","    loss_dis = loss_for_real + loss_for_fake\n","    return loss_dis\n","\n","def discriminator_loss_for_fake(D_out_fake):\n","    # Define loss_gen\n","    loss_gen = ?????\n","    return loss_gen"]},{"cell_type":"markdown","metadata":{"id":"LcInWogOW6aZ"},"source":["Let's design the cycle_consistency loss based on L1 loss\n","![](https://drive.google.com/uc?id=1FvVJ172ptRMXUaNRofGnV5eNu5tuu2HE)"]},{"cell_type":"markdown","metadata":{"id":"EYbpcJYRW6aZ"},"source":["### Let's design the cycle consistency loss!\n","\n","> **Exercise**: Create a cycle consistency loss using the above equation."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"x3duccIRW6aZ"},"outputs":[],"source":[" def cycle_consistency_loss(real_im, reconstructed_im):\n","    # real_im: x^A\n","    # reconstructed_im: G^B(G^A(x^A))\n","    # Define loss_cycle\n","\n","    return loss_cycle"]},{"cell_type":"markdown","metadata":{"id":"39Elp6ZkW6aZ"},"source":["## 3.2 Student Objective"]},{"cell_type":"markdown","metadata":{"id":"oRXzVVY2W6aZ"},"source":["### 3.2.1. Output distillation loss based on 'Style loss' and 'Feature reconstruction loss' using VGG features of  teacher and student outputs"]},{"cell_type":"markdown","metadata":{"id":"uiE4QM7vW6aZ"},"source":["First, define the Vgg Feature extractor as below"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"zIYFFVX6W6aZ"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from torchvision import models\n","\n","\n","class Vgg16(nn.Module):\n","    def __init__(self):\n","        super(Vgg16, self).__init__()\n","        features = models.vgg16(pretrained=True).features\n","        self.to_relu_1_2 = nn.Sequential()\n","        self.to_relu_2_2 = nn.Sequential()\n","        self.to_relu_3_3 = nn.Sequential()\n","        self.to_relu_4_3 = nn.Sequential()\n","\n","        for x in range(4):\n","            self.to_relu_1_2.add_module(str(x), features[x])\n","        for x in range(4, 9):\n","            self.to_relu_2_2.add_module(str(x), features[x])\n","        for x in range(9, 16):\n","            self.to_relu_3_3.add_module(str(x), features[x])\n","        for x in range(16, 23):\n","            self.to_relu_4_3.add_module(str(x), features[x])\n","\n","        # don't need the gradients, just want the features\n","        for param in self.parameters():\n","            param.requires_grad = False\n","\n","    def forward(self, x):\n","        h = self.to_relu_1_2(x)\n","        h_relu_1_2 = h\n","        h = self.to_relu_2_2(h)\n","        h_relu_2_2 = h\n","        h = self.to_relu_3_3(h)\n","        h_relu_3_3 = h\n","        h = self.to_relu_4_3(h)\n","        h_relu_4_3 = h\n","        out = (h_relu_1_2, h_relu_2_2, h_relu_3_3, h_relu_4_3)\n","        return out\n","\n","\n","class VGGFeature(nn.Module):\n","    def __init__(self):\n","        super(VGGFeature, self).__init__()\n","        self.add_module('vgg', Vgg16())\n","    def __call__(self,x):\n","        x = (x.clone()+1.)/2. # [-1, 1] -> [0,1] (Normalize the inpu data)\n","        x_vgg = self.vgg(x)\n","        return x_vgg"]},{"cell_type":"markdown","metadata":{"id":"6DZyPuBZW6aa"},"source":["Second, define the gram matrix of x, which is slightly differnet from the definition, so please refer to the following equation.\n","![](https://drive.google.com/uc?id=1moWsECgoMIU5P9Nd1esPaGKE4eB9Wtjg)"]},{"cell_type":"markdown","metadata":{"id":"QO8CMIqdW6aa"},"source":["### Let's implement the Gram matrix!\n","\n","> **Exercise**: Create a function to compute the Gram matrix using the above equation (Hint: use torch.bmm)."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"8jXWZEVdW6aa"},"outputs":[],"source":["def gram(x):\n","        (bs, ch, h, w) = x.size()\n","        # Defein G (Gram matrix)\n","        G = ?????\n","        return G"]},{"cell_type":"markdown","metadata":{"id":"C04mltJkW6aa"},"source":["Third, define the style loss based on the teacher generated image (t) and the student generated image (s) using the above Gram matrix as follows:\n","![](https://drive.google.com/uc?id=1C1Qv2n4Tqrzy_O2fL3ayCMarny7cXGRH)"]},{"cell_type":"markdown","metadata":{"id":"FgeKAL7fW6aa"},"source":["### Let's design the style loss!\n","\n","> **Exercise**: Create a style loss using the above equation."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"KzTTRjsMW6aa"},"outputs":[],"source":[" def style_loss_for_student(student_vgg_features, teacher_vgg_features):\n","    loss_G_style = 0\n","    for student_vgg_feature, teacher_vgg_feature in zip(student_vgg_features, teacher_vgg_features):\n","        # Define loss_G_style\n","        # You can regard student_vgg_feature as F_j(s)\n","        # You can regard teacher_vgg_feature as F_j(t)\n","        loss_G_style += ??????\n","    return loss_G_style"]},{"cell_type":"markdown","metadata":{"id":"8x14XuUQW6ab"},"source":["Finally, define the feature reconstruction loss which is given by\n","![](https://drive.google.com/uc?id=19RRhPPSbzNQI4nuCPmPRPs4v_MncF7No)\n"]},{"cell_type":"markdown","metadata":{"id":"-a9fwRfmW6ab"},"source":["### Let's design the feature reconstruction loss!\n","\n","> **Exercise**: Create a feature reconstruction loss using the above equation."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"euBaxhQcW6ab"},"outputs":[],"source":["def feature_recon_loss_for_student(student_vgg_feature, teacher_vgg_feature):\n","    # You can regard student_vgg_feature as F_j(s)\n","    # You can regard teacher_vgg_feature as F_j(t)\n","    loss_G_recon = ???\n","\n","    return loss_G_recon"]},{"cell_type":"markdown","metadata":{"id":"Od7YEQUcW6ab"},"source":["### 3.2.2. Total variation loss to smoothness of the generated image given by the student"]},{"cell_type":"markdown","metadata":{"id":"KlMDmkv0W6ab"},"source":["Let's define our total variation loss, which is slightly different from the true definition, where the variant total variation loss is given by\n","![](https://drive.google.com/uc?id=1T_n8keQjKjmklE60TFeZd0dmKh5WQEhr)"]},{"cell_type":"markdown","metadata":{"id":"HSEvmh3eW6ab"},"source":["### Let's design the total variation loss!\n","\n","> **Exercise**: Create a total variation loss using the above equation. Then, perform summation over each total variation loss over a mini-batch (Not important)"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"StPfpznlW6ab"},"outputs":[],"source":["def total_variation(student_im):\n","\n","    diff_i = ???\n","    diff_j = ???\n","    return diff_i + diff_j"]},{"cell_type":"markdown","metadata":{"id":"Wj5evqetW6ab"},"source":["### 3.2.3. Mutual Information Maximization Term"]},{"cell_type":"markdown","metadata":{"id":"rPCsQi2LW6ac"},"source":["The gradient of the mutual information with respect to the student generator is given by\n","![](https://drive.google.com/uc?id=18HukuvROsJSsTdBZzaBT9QObWhJPUHov)"]},{"cell_type":"markdown","metadata":{"id":"yQj8A21_W6ac"},"source":["### Let's design the negative mutual information loss\n","\n","> **Exercise**: Create the mutual information function such that the gradient of the loss can be expressed as the above equation."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"vICn_YK7W6ac"},"outputs":[],"source":["def negative_mutual_information(ebm, teacher_im, MCMC_im, student_im):\n","    \"\"\"\n","    Note that forward function in ebm takes two inputs\n","    fist_input: teacher or mcmc samples\n","    second_input: student samples\n","    \"\"\"\n","    loss_G_ebm = ????\n","    return loss_G_ebm"]},{"cell_type":"markdown","metadata":{"id":"Vo6YK0qnW6ac"},"source":["## 3.3 Energy-based Model Objective"]},{"cell_type":"markdown","metadata":{"id":"-2gqhx5OW6ac"},"source":["The energy-based model is learned to minimize the difference between the true mutual information and its variational bound, which is equivalent to solve the following optimization\n","\n","![](https://drive.google.com/uc?id=1R5dgtWLnsUKz2enCGeDv2jvYGpj0Mq8z)"]},{"cell_type":"markdown","metadata":{"id":"-6WDU7kwW6ac"},"source":["### Let's design the energy-based model objective\n","\n","> **Exercise**: Create the objective function using the above equation."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"1BEYgqrpW6ac"},"outputs":[],"source":["def objective_EBM(ebm, teacher_im, MCMC_im, student_im):\n","    # Define loss_EBM using the above equation\n","    first_output = ?\n","    second_output = ?\n","    loss_EBM = first_output - second_output\n","\n","    \"\"\"\n","    For training stability, we additionally apply L2 regularization to the output\n","    which is helpful to encourage the output to become bounded\n","    \"\"\"\n","    lambda_l2_coeff = 0.05\n","    loss_EBM += lambda_l2_coeff * (first_output **2 + second_output **2).mean()\n","    return loss_EBM"]},{"cell_type":"markdown","metadata":{"id":"VmxTo2EFW6ad"},"source":["### Let's design Langevin Dynamics\n","\n","> **Exercise**: Create MCMC function using the below equation.\n","![](https://drive.google.com/uc?id=1-EIuc_jtyRpKwiXRQBSs6Pf6OdDjxs74)"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"Fj0b9x_8W6ad"},"outputs":[],"source":["def MCMC(ebm, student_im, step_size=50.0, sigma=0.005, MCMC_steps=10):\n","    MCMC_img = # Initialization as student_im\n","\n","    for k in range(MCMC_steps):\n","        MCMC_img = ????????\n","        # using torch.autograd.grad\n","\n","    MCMC_img = MCMC_img.detach().clamp(min=-1.0, max=1.0)\n","    return MCMC_img"]},{"cell_type":"markdown","metadata":{"id":"_dBaAfotW6ad"},"source":["# 4. Optimizers"]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"ZWX-G_1JW6ad"},"source":["\n","- We will use Adam with a initial learning rate of ```lr=0.0002``` for Generators and Discriminators.\n","- We will use Adam with a initial learning rate of ```lr=0.0001``` for EBM.\n","\n","- In PyTorch, we pass the model's parameters into the optimizer. The model's parameters can be obtained by calling ```model.parameters()```.\n","\n","- When we want to optimize the parameters of two models with one optimizer, we can simply concatenate the list of the two models' parameters and pass that into the optimizer"]},{"cell_type":"markdown","metadata":{"id":"R49t2mG-W6ad"},"source":["### Let's create optimizers\n","\n","> **Exercise**: Create the optimizers."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"MTPNQqXZW6ad"},"outputs":[],"source":["import torch.optim as optim\n","# Hyperparameters for Adam optimizer of CycleGANs and the student generator\n","lr = 0.0002\n","beta1 = 0.5\n","beta2 = 0.999\n","\n","optimizer_G_student = torch.optim.Adam(netG_student.parameters(), lr=lr, betas=(beta1, beta2))\n","#\n","# Create a single optimizer for both generators\n","netG_params = ?\n","optimizer_G_teacher = torch.optim.Adam(netG_params, lr=lr, betas=(beta1, beta2))\n","\n","#\n","# Create a single optimizer for both discriminators\n","netD_params = ?\n","optimizer_D_teacher = torch.optim.Adam(netD_params, lr=lr, betas=(beta1,beta2))\n","\n","\n","# Hyperparameters for Adam optimizer of EBM\n","ebm_lr=0.0001\n","beta1=0.0\n","beta2=0.999\n","optimizer_EBM = torch.optim.Adam(netEBM.parameters(), lr=ebm_lr, betas=(beta1, beta2))\n",""]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"1pBiJHqPW6ad"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"4GLQF7URW6ad"},"source":["# 5. Training"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"KNc8iX8lW6ae"},"outputs":[],"source":["def set_requires_grad(net, requires_grad):\n","    \"\"\"Set requies_grad=Fasle for the network to avoid unnecessary computations\n","        Parameters:\n","            net                   -- network\n","            requires_grad (bool)  -- whether the networks require gradients or not\n","    \"\"\"\n","\n","    ????\n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"vOg2urxIW6ae"},"outputs":[],"source":["def optimize_teacher(data_i_A, data_i_B):\n","    # ============================================\n","    #            TRAIN Generators\n","    # ============================================\n","\n","\n","    # ============================================\n","    #            Forward\n","    # ============================================\n","    fake_B = ??? # G_A(data_i_A)\n","    rec_A = ??? # for cycle consistency loss that makes G_B(fake_B) close to data_i_A\n","    fake_A = ??? # G_B(data_i_B)\n","    rec_B = ??? # for cycle consistency loss that makes G_A(fake_A) close to data_i_B\n","\n","\n","    optimizer_G_teacher.zero_grad()\n","    optimizer_D_teacher.zero_grad()\n","    set_requires_grad(netD_teacher_A, False)\n","    set_requires_grad(netD_teacher_B, False)\n","\n","\n","    #            LSGAN Loss\n","    ???????\n","    lsgan_loss_A =\n","    lsgan_loss_B =\n","\n","    #            Cycle Consistency Loss\n","    ???????\n","    cycle_consistency_loss_A =\n","    cycle_consistency_loss_B =\n","\n","    #            Identity_Loss (Additionally we use the identity loss defined as below)\n","    # Motivation: Encourage to become close to the input image and the translated image\n","    # For preserving the background details\n","    loss_identity_A = torch.nn.functional.l1_loss(fake_B, data_i_A)\n","    loss_identity_B = torch.nn.functional.l1_loss(fake_A, data_i_B)\n","\n","\n","    #            Backward\n","    lambda_cycle_A, lambda_cycle_B = 10.0, 10.0\n","    lambda_idty = 10.\n","    loss_G_teacher_A = lsgan_loss_A + cycle_consistency_loss_A * lambda_cycle_A\n","    loss_G_teacher_B = lsgan_loss_B + cycle_consistency_loss_B * lambda_cycle_B\n","    loss_G_teacher = loss_G_teacher_A + loss_G_teacher_B + lambda_idty * (loss_identity_A + loss_identity_B)\n","\n","    loss_G_teacher.backward()\n","    optimizer_G_teacher.step()\n","\n","\n","    # ============================================\n","    #            TRAIN Discriminators\n","    # ============================================\n","    set_requires_grad(netD_teacher_A, True)\n","    set_requires_grad(netD_teacher_B, True)\n","\n","\n","\n","    #            LSGAN Loss for netD_teacher_A\n","    D_A_out_real = ?\n","    D_A_out_fake = ?\n","    lsgan_loss_A = ?\n","\n","    #            LSGAN Loss for netD_teacher_B\n","    D_B_out_real = ?\n","    D_B_out_fake = ?\n","    lsgan_loss_B = ?\n","\n","    #            Backward\n","    loss_D_teacher = 0.5 * lsgan_loss_A + 0.5 * lsgan_loss_B\n","    loss_D_teacher.backward()\n","    optimizer_D_teacher.step()\n","\n","\n","    return loss_G_teacher_A, loss_G_teacher_B, lsgan_loss_A, lsgan_loss_B\n",""]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"3dyUUF0hW6ae"},"outputs":[],"source":["\n","def optimize_student_ebm(VGGNet, student_dataloader, student_n_epoch, student_epoch, training_epochs):\n","    print_every=100\n","    lambda_style = 1e4\n","    lambda_feat = 1e1\n","    lambda_tv = 1e-5\n","    lambda_mi = 1e-1\n","    student_iter = 0\n","    for epoch in range(student_epoch, student_epoch + training_epochs):\n","        for i, data_i in enumerate(student_dataloader):\n","            student_iter += 1\n","            data_i_A = data_i['A'].to(device) # We ignore domain B data\n","            optimizer_G_student.zero_grad()\n","            optimizer_EBM.zero_grad()\n","\n","            set_requires_grad(netEBM, False)\n","\n","            # ============================================\n","            #            Forward\n","            # ============================================\n","            with torch.no_grad():\n","                Tfake = ???\n","            Sfake = ???\n","\n","            #            sampling energy-based model samples via Langevin Dynamics\n","            Tfake_MCMC = ?????\n","\n","\n","            # ============================================\n","            #            TRAIN EBM\n","            # ============================================\n","            set_requires_grad(netEBM, True)\n","            #            Energy-based Model Loss for netEBM\n","            ebm_loss =  ?\n","            ebm_loss.backward()\n","            optimizer_EBM.step()\n","            optimizer_EBM.zero_grad()\n","\n","\n","            # ============================================\n","            #            TRAIN Generator\n","            # ============================================\n","\n","            set_requires_grad(netEBM, False)\n","            set_requires_grad(netG_teacher_A, True)\n","\n","\n","            student_vgg_features = VGGNet(Sfake)\n","            teacher_vgg_features = VGGNet(Tfake)\n","\n","            #            Style Loss for netG_student\n","            style_loss = ???\n","\n","\n","            #            Feature Reconstruction Loss for netG_student\n","            student_vgg_feature = student_vgg_features[1]\n","            teacher_vgg_feature = teacher_vgg_features[1]\n","            feat_loss = ??\n","\n","            #            Total Variation Loss for netG_student\n","            total_variation_loss = ???\n","\n","            #            Negative Mutual Information Loss for netG_student\n","            negative_mi = ????\n","\n","\n","            student_loss = lambda_style * style_loss + lambda_feat * feat_loss + \\\n","            lambda_tv * total_variation_loss + lambda_mi * negative_mi\n","\n","            student_loss.backward()\n","            optimizer_G_student.step()\n","\n","            if student_iter % print_every == 0:\n","                print ('Student Loss')\n","                print('Epoch [{:5d}/{:5d}] | EBM_loss: {:6.4f} | Student_loss: {:6.4f}'.format(\n","epoch, student_n_epoch, ebm_loss.item(), student_loss.item()))\n",""]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"eDvYhFppW6ae"},"outputs":[],"source":["from util import save_network, get_scheduler, update_learning_rate\n","\n","MODEL_NAMES=['netG_teacher_A', 'netG_teacher_B', 'netG_student', 'netD_teacher_A', 'netD_teacher_B', 'netEBM']\n","MODELS =[netG_teacher_A, netG_teacher_B, netG_student, netD_teacher_A, netD_teacher_B, netEBM]\n","OPTIMIZERS=[optimizer_G_student, optimizer_G_teacher, optimizer_D_teacher, optimizer_EBM]\n","SCHEDULERS = get_scheduler(OPTIMIZERS, 100, 100)\n","def training_loop(dataloader, student_dataloader, n_epochs=200):\n","    import time\n","    since = time.time()\n","    print_every=100\n","    sample_every=100\n","    save_latest_freq=25000\n","\n","    losses = []      # keep track of losses over time\n","\n","\n","    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","    total_iter = 0\n","    student_epoch_period = 10\n","    student_epochs_per_period = 40\n","    student_epoch = 1\n","    student_n_epoch = (n_epochs//student_epoch_period) * student_epochs_per_period\n","    VGGNet = VGGFeature().to(device)\n","    for epoch in range(1, n_epochs+1):\n","\n","        # Reset iterators for each epoch\n","        #if epoch % batches_per_epoch == 0:\n","            #iter_data = iter(dataloader)\n","        for i, data_i in enumerate(dataloader):\n","            total_iter += 1\n","            # move images to device\n","            data_i_A = data_i['A'].to(device)\n","            data_i_B = data_i['B'].to(device)\n","\n","\n","            \"\"\"       Teacher Training       \"\"\"\n","            loss_G_teacher_A, loss_G_teacher_B, loss_D_teacher_A, loss_D_teacher_B = \\\n","            optimize_teacher(data_i_A, data_i_B)\n","\n","            # Print\n","            if total_iter % print_every == 0:\n","                time_elapsed = time.time() - since\n","                print ('Teacher Loss')\n","                print('Epoch [{:5d}/{:5d}] | d_A_loss: {:6.4f} | d_B_loss: {:6.4f} | g_A_loss: {:6.4f} | g_B_loss: {:6.4f} time : {:.0f}m {:.0f}s'.format(\n","                    epoch, n_epochs, loss_D_teacher_A.item(), loss_D_teacher_B.item(),loss_G_teacher_A.item(), loss_G_teacher_B.item() ,time_elapsed // 60, time_elapsed % 60))\n","\n","            if total_iter % save_latest_freq == 0:\n","                save_network(MODELS, MODEL_NAMES, 'iter_%d'%total_iter, './checkpoint')\n","\n","            \"\"\"\n","            checkpoint_every=1\n","            if epoch % checkpoint_every == 0:\n","                checkpoint(epoch, G_XtoY, G_YtoX, D_X, D_Y)\n","            \"\"\"\n","\n","\n","        if epoch % student_epoch_period == 0:\n","            optimize_student_ebm(VGGNet, student_dataloader, student_n_epoch, student_epoch, student_epochs_per_period)\n","            student_epoch += student_epochs_per_period\n","\n","\n","        update_learning_rate(SCHEDULERS, OPTIMIZERS)\n","\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"O5JKrDbHW6af"},"outputs":[],"source":["n_epochs=200\n","dataloader = create_dataloader(dataroot='./database/horse2zebra', shuffle=True, batch_size=1, num_workers=4)\n","student_dataloader = create_dataloader(dataroot='./database/horse2zebra', shuffle=True, batch_size=1, num_workers=4)\n","training_loop(dataloader, student_dataloader, n_epochs)"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"bG9BW7LJW6af"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"WOmqjQRBW6af"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"OzX9nRAfW6af"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"lNFOzvPpW6af"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"0JO-ghL1W6af"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"rsKiiTZ-W6af"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}