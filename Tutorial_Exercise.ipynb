{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VEM combined with OMGD on Horse2Zebra using CycleGAN \n",
    "\n",
    "\n",
    ">## OMGD\n",
    "\n",
    ">* OMGD is an online distillation framework which jointly trains a teacher and a student. \n",
    "\n",
    ">## Teacher Training using CycleGAN objective\n",
    "<img src='imgs/CycleGAN.png' width=70% />\n",
    "\n",
    "> * The generator is trained to fool the discriminator, it wants to output data that looks _as close as possible_ to real, training data. \n",
    "* The discriminator is a classifier that is trained to figure out which data is real and which is fake. \n",
    "\n",
    ">## Student and Energy-based Model Training\n",
    "<img src='imgs/NeurIPS2022_VEM_Framework.png' width=70% />\n",
    "\n",
    "> * EBM is optimized to minimize the difference between the true mutual information and its lower bound esimated by EBM.\n",
    "\n",
    "> * Student is trained to minimize the intermediate feature map and output distillation losses while maximizing the mutual information between the teacher and the studnet. In addition, we learn the student to minimize the total variation loss for spatial smoothness.\n",
    "\n",
    "> * In this lecture, we do not implement the inintermediate feature map distillation loss, so if you want to reproduce the algorithm, please run mi_distill.sh in `./scripts/cycle_gan/horse2zebra`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 0. Setup for downloading horse2zebra dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Download horse-zebra dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gdown \n",
    "import os\n",
    "file_id = \"1wMiqnCAPlXvhmpvT5x65T007fFrjRW1D\"\n",
    "os.makedirs('./database', exist_ok=True)\n",
    "gdown.download(id=file_id,output='./database/horse2zebra.zip', quiet=False)\n",
    "os.system(\"unzip ./database/horse2zebra.zip -d ./database\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Construct the unaligned horse2zebra dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "import os\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "IMG_EXTENSIONS = [\n",
    "    '.jpg', '.JPG', '.jpeg', '.JPEG',\n",
    "    '.png', '.PNG', '.ppm', '.PPM', '.bmp', '.BMP',\n",
    "]\n",
    "def make_dataset(dir, max_dataset_size=float('inf')): \n",
    "    assert os.path.isdir(dir) or os.path.islink(dir), '%s is not a valid directory' % dir\n",
    "    images = []\n",
    "    for root, dnames, fnames in sorted(os.walk(dir)): \n",
    "        for fname in fnames:\n",
    "            if any(fname.endswith(extension) for extension in IMG_EXTENSIONS):\n",
    "                path = os.path.join(root, fname)\n",
    "                images.append(path)\n",
    "    return images[:min(max_dataset_size, len(images))]\n",
    "    \n",
    "class UnalignedDataset(data.Dataset):\n",
    "    \"\"\"\n",
    "    This dataset class can load unaligned/unpaired datasets.\n",
    "\n",
    "    It requires two directories to host training images from domain A '/path/to/data/trainA'\n",
    "    and from domain B '/path/to/data/trainB' respectively.\n",
    "    You can train the model with the dataset flag '--dataroot /path/to/data'.\n",
    "    Similarly, you need to prepare two directories:\n",
    "    '/path/to/data/testA' and '/path/to/data/testB' during test time.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataroot, phase='train', load_size=286, size=256):\n",
    "        self.dir_A = os.path.join(dataroot, phase + 'A') # \"./daabase/horse2zebra/trainA\"\n",
    "        self.dir_B = os.path.join(dataroot, phase + 'B') # \"./daabase/horse2zebra/trainB\"\n",
    "        self.A_paths = sorted(make_dataset(self.dir_A)) \n",
    "        # self.A_paths: List[str] = ['./daabase/horse2zebra/trainA/name1.jpg', './daabase/horse2zebra/trainA/name2.jpg', ..., './daabase/horse2zebra/trainA/nameN.jpg']  \n",
    "        self.B_paths = sorted(make_dataset(self.dir_B))\n",
    "        # self.B_paths: List[str] = ['./daabase/horse2zebra/trainB/name1.jpg', './daabase/horse2zebra/trainB/name2.jpg', ..., './daabase/horse2zebra/trainB/nameN.jpg']  \n",
    "        self.A_size = len(self.A_paths)\n",
    "        self.B_size = len(self.B_paths)\n",
    "        self.transform_A = self._get_transform(load_size, size)\n",
    "        self.transform_B = self._get_transform(load_size, size)\n",
    "        \n",
    "    \n",
    "    def _get_transform(self, load_size=286, size=256):\n",
    "        \"\"\"\n",
    "        Resize an input image as load_size x load_size and then crop the resized image to size x size\n",
    "        \"\"\"\n",
    "        transform_list = []\n",
    "        resize = [load_size, load_size]\n",
    "        transform_list.append(transforms.Resize(resize, Image.BICUBIC)) # Resize the input image as load_size x load_size\n",
    "        transform_list.append(transforms.RandomCrop(size)) # Randomly crop the resized image to size x size \n",
    "        transform_list.append(transforms.ToTensor()) # Make the cropped image whose range is in [0,1] \n",
    "        transform_list.append(transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))) # Normalize the image in [-1, 1]\n",
    "        \n",
    "        return transforms.Compose(transform_list)\n",
    "    \n",
    "    def __getitem__(self, idx_A): \n",
    "        \"\"\"Return a data point and its metadata information.\n",
    "\n",
    "        Parameters:\n",
    "            index (int)      -- a random integer for data indexing\n",
    "\n",
    "        Returns a dictionary that contains A, B, A_path and B_path\n",
    "            A (tensor)       -- an image in the input domain\n",
    "            B (tensor)       -- an image in the target domain\n",
    "            A_path (str)    -- image path for domain A\n",
    "            B_path (str)    -- image path for domain B\n",
    "        \"\"\"\n",
    "        \n",
    "        A_path = self.A_paths[idx_A % self.A_size] # make sure index is within then range\n",
    "        idx_B = random.randint(0, self.B_size - 1) # randomize the index for domain B to avoid fixed pairs.\n",
    "        B_path = self.B_paths[idx_B]\n",
    "        A_img = Image.open(A_path).convert('RGB') # Read image using A_path \n",
    "        B_img = Image.open(B_path).convert('RGB') # Read image using B_path\n",
    "        # apply image transformation\n",
    "        A = self.transform_A(A_img) # use transform_A\n",
    "        B = self.transform_B(B_img) # use transform_B\n",
    "        return {'A': A, 'B': B, 'A_paths': A_path, 'B_paths': B_path}\n",
    "    \n",
    "    def __len__(self):\n",
    "        # retunr the maximum size of dataset A and B\n",
    "        return max(self.A_size, self.B_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the data\n",
    "We define an inverse transform that changes a tensor back to a PIL Image. First, we \"unnormalize\" the tensor by taking the inverse of the normalization function from above. Then, we can call ```transforms.ToPILImage()``` which will convert the tensor to a PIL Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Some matplotlib settings\n",
    "plt.rcParams[\"figure.figsize\"] = (16, 10)\n",
    "plt.rcParams[\"axes.grid\"] = False\n",
    "plt.rcParams[\"xtick.major.bottom\"] = False\n",
    "plt.rcParams[\"ytick.major.left\"] = False\n",
    "def get_pil_img_from_tensor(img_tensor):\n",
    "    # incerse_transform: make tensor whose range in [-1,1] -> PIL Image whose range in uint8 ([0,255])\n",
    "    inverse_transform = transforms.Compose([transforms.Normalize(mean=[-1, -1, -1], std=[1/0.5, 1/0.5, 1/0.5]), \n",
    "                                            transforms.ToPILImage()])\n",
    "    return inverse_transform(img_tensor)\n",
    "\n",
    "def show_img(pil_img):\n",
    "    plt.imshow(pil_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "dataroot = './database/horse2zebra'\n",
    "horse2zebra_dataset = UnalignedDataset(dataroot= dataroot)\n",
    "visualization_horse_data = torch.cat([horse2zebra_dataset[i]['A'].unsqueeze(0) for i in range(16)], 0)\n",
    "show_img(get_pil_img_from_tensor(torchvision.utils.make_grid(visualization_horse_data)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataroot = './database/horse2zebra'\n",
    "visualization_zebra_data = torch.cat([horse2zebra_dataset[i]['B'].unsqueeze(0) for i in range(16)], 0)\n",
    "show_img(get_pil_img_from_tensor(torchvision.utils.make_grid(visualization_zebra_data)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Dataset and generate dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "# create DataLoader\n",
    "def create_dataloader(dataroot='./database/horse2zebra', shuffle=True, batch_size=1, num_workers=4):\n",
    "    horse2zebra_dataset = UnalignedDataset(dataroot= dataroot)\n",
    "    dataloader = DataLoader(dataset=horse2zebra_dataset, \n",
    "                              batch_size=batch_size, \n",
    "                              shuffle=shuffle, \n",
    "                              num_workers=num_workers)\n",
    "    return dataloader \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Define the model\n",
    "## Component: \n",
    "\n",
    "> ### Two discriminators (A->B and B->A)\n",
    "> ### Two teacher generators (A->B and B->A)\n",
    "> ### One student generator (A->B)\n",
    "> ### One enrgy-based model for the student"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define discriminator for training a teacher generator (Norm: nn.InstanceNorm2d) \n",
    "<img src='imgs/Samsung_AI_Expert_Discriminator_Structure.png' width=100% />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's create the Discriminator!\n",
    "\n",
    "> **Exercise**: Create a generator model using nn.Conv2d(padding_mode='zeros'), nn.LeakyReLU, and nn.InstanceNorm2d function. Refer to the image above for what the network should look like!\n",
    "\n",
    "\n",
    "> Refer to nn.Conv2d output size formula:\n",
    "```\n",
    "output_size = 1 + (input_size - kernel_size + 2*padding) / stride\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"Defines a PatchGAN discriminator\"\"\"\n",
    "\n",
    "    def __init__(self, input_nc=3, ndf=128, n_layers=3, norm_layer=nn.InstanceNorm2d):\n",
    "        \"\"\"Construct a PatchGAN discriminator\n",
    "\n",
    "        Parameters:\n",
    "            input_nc (int)  -- the number of channels in input images\n",
    "            ndf (int)       -- the number of filters in the last conv layer\n",
    "            n_layers (int)  -- the number of conv layers in the discriminator\n",
    "            norm_layer      -- normalization layer\n",
    "        \"\"\"\n",
    "        super(Discriminator, self).__init__()\n",
    "        # Define self.model\n",
    "        ??????\n",
    "        \n",
    "    def forward(self, input):\n",
    "       \n",
    "        return self.model(input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\"\"\"\n",
    "Test Code: Check your implementation\n",
    "\"\"\"\n",
    "\n",
    "with torch.no_grad():\n",
    "    Test_D = Discriminator(input_nc=3,)\n",
    "    input_ = torch.randn(1,3,256,256)\n",
    "    output = Test_D(input_)\n",
    "    print (output.size())\n",
    "    assert output.size() == (1,1,30,30), \"check the network output shape\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define generator based on a residual structure (Norm: nn.InstanceNorm2d)  \n",
    "<img src='imgs/Samsung_AI_Expert_Generator_Structure.png' width=100% />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Define the MobileResidualBlock (Norm: nn.InstanceNorm2d)\n",
    "<img src='imgs/Samsung_AI_Expert_MobileResnetBlock_Structure.png' width=100% />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's create the Generator!\n",
    "\n",
    "> **Exercise**: Create a generator model using nn.Conv2d(padding_mode='reflect'), nn.ConvTranspose2d, nn.ReLU, and nn.InstanceNorm2d function. Refer to the image above for what the network should look like!\n",
    "\n",
    "> nn.Conv2d output size formula:\n",
    "```\n",
    "output_size = 1 + (input_size - kernel_size + 2*padding) // stride\n",
    "```\n",
    "\n",
    ">nn.ConvTranspose2d size formula\n",
    "```\n",
    "output_size = 1 + (input_size − 1) × stride − 2×padding + (kernel_size - 1) + output_padding\n",
    "```\n",
    "\n",
    ">In case of depthwise cconvolution, you can employ nn.Conv2d(groups=in_channels), where 'in_channels' denotes the number of the input feature maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class SeparableConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, padding, norm_layer, stride=1, scale_factor=1):\n",
    "        super(SeparableConv2d, self).__init__()\n",
    "        # Define\n",
    "        ?????  self.conv conists of 3x3 depthwise conv, norm and 1x1 conv \n",
    "        self.conv =  \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class MobileResnetBlock(nn.Module):\n",
    "    def __init__(self, dim, norm_layer):\n",
    "        super(MobileResnetBlock, self).__init__()\n",
    "        \n",
    "        conv_block = [\n",
    "            SeparableConv2d(????),\n",
    "            norm????, ReLU???\n",
    "        ]\n",
    "\n",
    "        conv_block += [\n",
    "            SeparableConv2d(????)\n",
    "             norm???\n",
    "        ]\n",
    "\n",
    "        self.conv_block = nn.Sequential(*conv_block)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x + self.conv_block(x)\n",
    "        return out\n",
    "    \n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_nc, output_nc, ngf, norm_layer=nn.InstanceNorm2d, n_blocks=9):\n",
    "        super(Generator, self).__init__()\n",
    "        #Define \n",
    "        #Note that self.model which has n_blocks for MobileResnetBLock\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return self.model(input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\"\"\"\n",
    "Test Code: Check your implementation\n",
    "\"\"\"\n",
    "with torch.no_grad():\n",
    "    Test_G = Generator(input_nc=3, output_nc=3, ngf=64)\n",
    "    input_ = torch.randn(1,3,256,256)\n",
    "    output = Test_G(input_)\n",
    "    print (output.size())\n",
    "    assert output.size() == (1,3,256,256), \"check the network output shape\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define energy-based model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from util import ResnetEBM\n",
    "with torch.no_grad(): \n",
    "    Test_ebm = ResnetEBM(8, 7)\n",
    "    t_input = torch.randn([1,3,256,256])\n",
    "    s_input = torch.randn([1,3,256,256])\n",
    "    output = Test_ebm(t_input, s_input) \n",
    "    print (output.size())\n",
    "    assert output.size() == (1,1), \"check the network output shape\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Creator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model(Teacher_G_ngf=64, Student_G_ngf=16, Teacher_D_ndf=128, ebm_input_nc=8, device='cuda'):\n",
    "\n",
    "    # Instantiate Teacher (A->B & B->A) and Student (A->B)) generators\n",
    "    netG_teacher_A = Generator(input_nc=3, output_nc=3, ngf=Teacher_G_ngf, norm_layer=nn.InstanceNorm2d, n_blocks=9) # (A->B)\n",
    "    netG_teacher_B = Generator(input_nc=3, output_nc=3, ngf=Teacher_G_ngf, norm_layer=nn.InstanceNorm2d, n_blocks=9) # (B->A)\n",
    "    netG_student = Generator(input_nc=3, output_nc=3, ngf=Student_G_ngf, norm_layer=nn.InstanceNorm2d, n_blocks=9) # (A->B)\n",
    "     \n",
    "    # Instantiate Discriminators for training Teacher Generators\n",
    "    netD_teacher_A = Discriminator(input_nc=3, ndf=Teacher_D_ndf) # netD_teacher_A v.s. netG_teacher_A\n",
    "    netD_teacher_B = Discriminator(input_nc=3, ndf=Teacher_D_ndf) # netD_teacher_B v.s. netG_teacher_B\n",
    "    \n",
    "    \n",
    "    # Instantiate Energy-based Model for mutual information maximization\n",
    "    netEBM = ResnetEBM(nec=ebm_input_nc, n_blocks=7) #\n",
    "    \n",
    "    # Cast to appropriate device. \n",
    "    netG_teacher_A.to(device)\n",
    "    netG_teacher_A.train()\n",
    "    netG_teacher_B.to(device)\n",
    "    netG_teacher_B.train()\n",
    "    netG_student.to(device)\n",
    "    netG_student.train()\n",
    "    netD_teacher_A.to(device)\n",
    "    netD_teacher_A.train()\n",
    "    netD_teacher_B.to(device)\n",
    "    netD_teacher_B.train()\n",
    "    netEBM.to(device)\n",
    "    netEBM.train()\n",
    "    print('Models loaded on {}'.format(device))\n",
    "\n",
    "    return netG_teacher_A, netG_teacher_B, netG_student, netD_teacher_A, netD_teacher_B, netEBM\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Device Settings\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "netG_teacher_A, netG_teacher_B, netG_student, netD_teacher_A, netD_teacher_B, netEBM = \\\n",
    "create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Define the loss functions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Teacher Objective\n",
    "<img src='imgs/CycleGAN.png' width=70% />\n",
    "\n",
    "> * The generator is trained to fool the discriminator, it wants to output data that looks _as close as possible_ to real, training data. \n",
    "* The discriminator is a classifier that is trained to figure out which data is real and which is fake."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use LSGAN loss for training teacher discriminators instead of the logistic loss.\n",
    "\n",
    "Let's design the discriminator loss using the following LSGAN loss equation \n",
    "<img src='imgs/LSGAN_Loss.png' width=60% />\n",
    "\n",
    "But, our discriminator output is not a scalar value. To address the issue, we naively apply the LSGAN loss to the discriminator output at each pixel and then average them over all pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's design the LSGAN loss!\n",
    "\n",
    "> **Exercise**: Create a LSGAN loss using the above equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discriminator_lsgan_loss(D_out_real, D_out_fake): \n",
    "    # Define loss_for_real and loss_for_fake\n",
    "    loss_for_real = ????\n",
    "    loss_for_fake = ????\n",
    "    loss_dis = loss_for_real + loss_for_fake \n",
    "    return loss_dis\n",
    "    \n",
    "def discriminator_loss_for_fake(D_out_fake):\n",
    "    # Define loss_gen\n",
    "    loss_gen = ?????\n",
    "    return loss_gen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's design the cycle_consistency loss based on L1 loss \n",
    "<img src='imgs/cycle_consistency.png' width=40% />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's design the cycle consistency loss!\n",
    "\n",
    "> **Exercise**: Create a cycle consistency loss using the above equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " def cycle_consistency_loss(real_im, reconstructed_im):\n",
    "    # real_im: x^A \n",
    "    # reconstructed_im: G^B(G^A(x^A)) \n",
    "    # Define loss_cycle\n",
    "    \n",
    "    return loss_cycle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Student Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1. Output distillation loss based on 'Style loss' and 'Feature reconstruction loss' using VGG features of  teacher and student outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, define the Vgg Feature extractor as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "\n",
    "class Vgg16(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Vgg16, self).__init__()\n",
    "        features = models.vgg16(pretrained=True).features\n",
    "        self.to_relu_1_2 = nn.Sequential()\n",
    "        self.to_relu_2_2 = nn.Sequential()\n",
    "        self.to_relu_3_3 = nn.Sequential()\n",
    "        self.to_relu_4_3 = nn.Sequential()\n",
    "\n",
    "        for x in range(4):\n",
    "            self.to_relu_1_2.add_module(str(x), features[x])\n",
    "        for x in range(4, 9):\n",
    "            self.to_relu_2_2.add_module(str(x), features[x])\n",
    "        for x in range(9, 16):\n",
    "            self.to_relu_3_3.add_module(str(x), features[x])\n",
    "        for x in range(16, 23):\n",
    "            self.to_relu_4_3.add_module(str(x), features[x])\n",
    "\n",
    "        # don't need the gradients, just want the features\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.to_relu_1_2(x)\n",
    "        h_relu_1_2 = h\n",
    "        h = self.to_relu_2_2(h)\n",
    "        h_relu_2_2 = h\n",
    "        h = self.to_relu_3_3(h)\n",
    "        h_relu_3_3 = h\n",
    "        h = self.to_relu_4_3(h)\n",
    "        h_relu_4_3 = h\n",
    "        out = (h_relu_1_2, h_relu_2_2, h_relu_3_3, h_relu_4_3)\n",
    "        return out\n",
    "\n",
    "\n",
    "class VGGFeature(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGGFeature, self).__init__()\n",
    "        self.add_module('vgg', Vgg16())\n",
    "    def __call__(self,x):\n",
    "        x = (x.clone()+1.)/2. # [-1, 1] -> [0,1] (Normalize the inpu data)\n",
    "        x_vgg = self.vgg(x)\n",
    "        return x_vgg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, define the gram matrix of x, which is slightly differnet from the definition, so please refer to the following equation.\n",
    "<img src='imgs/Gram_Matrix.png' width=70% />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's implement the Gram matrix!\n",
    "\n",
    "> **Exercise**: Create a function to compute the Gram matrix using the above equation (Hint: use torch.bmm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gram(x):\n",
    "        (bs, ch, h, w) = x.size()\n",
    "        # Defein G (Gram matrix)\n",
    "        G = ?????\n",
    "        return G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third, define the style loss based on the teacher generated image (t) and the student generated image (s) using the above Gram matrix as follows: \n",
    "<img src='imgs/style_loss.png' width=70% />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's design the style loss!\n",
    "\n",
    "> **Exercise**: Create a style loss using the above equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " def style_loss_for_student(student_vgg_features, teacher_vgg_features):\n",
    "    loss_G_style = 0\n",
    "    for student_vgg_feature, teacher_vgg_feature in zip(student_vgg_features, teacher_vgg_features):\n",
    "        # Define loss_G_style \n",
    "        # You can regard student_vgg_feature as F_j(s) \n",
    "        # You can regard teacher_vgg_feature as F_j(t)\n",
    "        loss_G_style += ??????\n",
    "    return loss_G_style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, define the feature reconstruction loss which is given by\n",
    "<img src='imgs/feat_recon_loss.png' width=70% />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's design the feature reconstruction loss!\n",
    "\n",
    "> **Exercise**: Create a feature reconstruction loss using the above equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_recon_loss_for_student(student_vgg_feature, teacher_vgg_feature):\n",
    "    # You can regard student_vgg_feature as F_j(s) \n",
    "    # You can regard teacher_vgg_feature as F_j(t)\n",
    "    loss_G_recon = ???\n",
    "    \n",
    "    return loss_G_recon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2. Total variation loss to smoothness of the generated image given by the student"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define our total variation loss, which is slightly different from the true definition, where the variant total variation loss is given by\n",
    "<img src='imgs/total_variation.png' width=50% />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's design the total variation loss!\n",
    "\n",
    "> **Exercise**: Create a total variation loss using the above equation. Then, perform summation over each total variation loss over a mini-batch (Not important)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def total_variation(student_im):\n",
    "   \n",
    "    diff_i = ???\n",
    "    diff_j = ???\n",
    "    return diff_i + diff_j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3. Mutual Information Maximization Term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient of the mutual information with respect to the student generator is given by \n",
    "<img src='imgs/mutual_information.png' width=70% />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's design the negative mutual information loss\n",
    "\n",
    "> **Exercise**: Create the mutual information function such that the gradient of the loss can be expressed as the above equation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def negative_mutual_information(ebm, teacher_im, MCMC_im, student_im):\n",
    "    \"\"\"\n",
    "    Note that forward function in ebm takes two inputs \n",
    "    fist_input: teacher or mcmc samples\n",
    "    second_input: student samples\n",
    "    \"\"\"\n",
    "    loss_G_ebm = ????\n",
    "    return loss_G_ebm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Energy-based Model Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The energy-based model is learned to minimize the difference between the true mutual information and its variational bound, which is equivalent to solve the following optimization\n",
    "\n",
    "<img src='imgs/Energymodel.png' width=70% />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's design the energy-based model objective\n",
    "\n",
    "> **Exercise**: Create the objective function using the above equation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def objective_EBM(ebm, teacher_im, MCMC_im, student_im):\n",
    "    # Define loss_EBM using the above equation\n",
    "    first_output = ?\n",
    "    second_output = ?\n",
    "    loss_EBM = first_output - second_output\n",
    "    \n",
    "    \"\"\"\n",
    "    For training stability, we additionally apply L2 regularization to the output\n",
    "    which is helpful to encourage the output to become bounded\n",
    "    \"\"\"\n",
    "    lambda_l2_coeff = 0.05\n",
    "    loss_EBM += lambda_l2_coeff * (first_output **2 + second_output **2).mean()\n",
    "    return loss_EBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's design Langevin Dynamics\n",
    "\n",
    "> **Exercise**: Create MCMC function using the below equation. \n",
    "<img src='imgs/MCMC.png' width=70% />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MCMC(ebm, student_im, step_size=50.0, sigma=0.005, MCMC_steps=10):\n",
    "    MCMC_img = # Initialization as student_im\n",
    "\n",
    "    for k in range(MCMC_steps):\n",
    "        MCMC_img = ???????? \n",
    "        # using torch.autograd.grad\n",
    "        \n",
    "    MCMC_img = MCMC_img.detach().clamp(min=-1.0, max=1.0)\n",
    "    return MCMC_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "- We will use Adam with a initial learning rate of ```lr=0.0002``` for Generators and Discriminators.\n",
    "- We will use Adam with a initial learning rate of ```lr=0.0001``` for EBM.\n",
    "\n",
    "- In PyTorch, we pass the model's parameters into the optimizer. The model's parameters can be obtained by calling ```model.parameters()```. \n",
    "\n",
    "- When we want to optimize the parameters of two models with one optimizer, we can simply concatenate the list of the two models' parameters and pass that into the optimizer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's create optimizers\n",
    "\n",
    "> **Exercise**: Create the optimizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "# Hyperparameters for Adam optimizer of CycleGANs and the student generator\n",
    "lr = 0.0002\n",
    "beta1 = 0.5\n",
    "beta2 = 0.999\n",
    "\n",
    "optimizer_G_student = torch.optim.Adam(netG_student.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "#\n",
    "# Create a single optimizer for both generators\n",
    "netG_params = ?\n",
    "optimizer_G_teacher = torch.optim.Adam(netG_params, lr=lr, betas=(beta1, beta2))\n",
    "\n",
    "#\n",
    "# Create a single optimizer for both discriminators\n",
    "netD_params = ?\n",
    "optimizer_D_teacher = torch.optim.Adam(netD_params, lr=lr, betas=(beta1,beta2))\n",
    "\n",
    "\n",
    "# Hyperparameters for Adam optimizer of EBM\n",
    "ebm_lr=0.0001 \n",
    "beta1=0.0 \n",
    "beta2=0.999\n",
    "optimizer_EBM = torch.optim.Adam(netEBM.parameters(), lr=ebm_lr, betas=(beta1, beta2))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def set_requires_grad(net, requires_grad):\n",
    "    \"\"\"Set requies_grad=Fasle for the network to avoid unnecessary computations\n",
    "        Parameters:\n",
    "            net                   -- network \n",
    "            requires_grad (bool)  -- whether the networks require gradients or not\n",
    "    \"\"\"\n",
    "    \n",
    "    ???? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimize_teacher(data_i_A, data_i_B):\n",
    "    # ============================================\n",
    "    #            TRAIN Generators\n",
    "    # ============================================\n",
    "    \n",
    "\n",
    "    # ============================================\n",
    "    #            Forward\n",
    "    # ============================================\n",
    "    fake_B = ??? # G_A(data_i_A)\n",
    "    rec_A = ??? # for cycle consistency loss that makes G_B(fake_B) close to data_i_A\n",
    "    fake_A = ??? # G_B(data_i_B)\n",
    "    rec_B = ??? # for cycle consistency loss that makes G_A(fake_A) close to data_i_B\n",
    "    \n",
    "    \n",
    "    optimizer_G_teacher.zero_grad()\n",
    "    optimizer_D_teacher.zero_grad()\n",
    "    set_requires_grad(netD_teacher_A, False)\n",
    "    set_requires_grad(netD_teacher_B, False)\n",
    "    \n",
    "    \n",
    "    #            LSGAN Loss\n",
    "    ??????? \n",
    "    lsgan_loss_A =\n",
    "    lsgan_loss_B = \n",
    "    \n",
    "    #            Cycle Consistency Loss\n",
    "    ??????? \n",
    "    cycle_consistency_loss_A =  \n",
    "    cycle_consistency_loss_B =\n",
    "    \n",
    "    #            Identity_Loss (Additionally we use the identity loss defined as below)\n",
    "    # Motivation: Encourage to become close to the input image and the translated image\n",
    "    # For preserving the background details\n",
    "    loss_identity_A = torch.nn.functional.l1_loss(fake_B, data_i_A)\n",
    "    loss_identity_B = torch.nn.functional.l1_loss(fake_A, data_i_B)\n",
    "    \n",
    "    \n",
    "    #            Backward\n",
    "    lambda_cycle_A, lambda_cycle_B = 10.0, 10.0\n",
    "    lambda_idty = 10.\n",
    "    loss_G_teacher_A = lsgan_loss_A + cycle_consistency_loss_A * lambda_cycle_A\n",
    "    loss_G_teacher_B = lsgan_loss_B + cycle_consistency_loss_B * lambda_cycle_B\n",
    "    loss_G_teacher = loss_G_teacher_A + loss_G_teacher_B + lambda_idty * (loss_identity_A + loss_identity_B)\n",
    "\n",
    "    loss_G_teacher.backward() \n",
    "    optimizer_G_teacher.step()\n",
    "   \n",
    "    \n",
    "    # ============================================\n",
    "    #            TRAIN Discriminators\n",
    "    # ============================================\n",
    "    set_requires_grad(netD_teacher_A, True)\n",
    "    set_requires_grad(netD_teacher_B, True)\n",
    "\n",
    "    \n",
    "    \n",
    "    #            LSGAN Loss for netD_teacher_A\n",
    "    D_A_out_real = ?\n",
    "    D_A_out_fake = ?\n",
    "    lsgan_loss_A = ?\n",
    "    \n",
    "    #            LSGAN Loss for netD_teacher_B\n",
    "    D_B_out_real = ?\n",
    "    D_B_out_fake = ?\n",
    "    lsgan_loss_B = ?\n",
    "    \n",
    "    #            Backward \n",
    "    loss_D_teacher = 0.5 * lsgan_loss_A + 0.5 * lsgan_loss_B\n",
    "    loss_D_teacher.backward()\n",
    "    optimizer_D_teacher.step()\n",
    "    \n",
    "    \n",
    "    return loss_G_teacher_A, loss_G_teacher_B, lsgan_loss_A, lsgan_loss_B\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def optimize_student_ebm(VGGNet, student_dataloader, student_n_epoch, student_epoch, training_epochs):\n",
    "    print_every=100\n",
    "    lambda_style = 1e4\n",
    "    lambda_feat = 1e1\n",
    "    lambda_tv = 1e-5\n",
    "    lambda_mi = 1e-1\n",
    "    student_iter = 0\n",
    "    for epoch in range(student_epoch, student_epoch + training_epochs):  \n",
    "        for i, data_i in enumerate(student_dataloader):\n",
    "            student_iter += 1\n",
    "            data_i_A = data_i['A'].to(device) # We ignore domain B data\n",
    "            optimizer_G_student.zero_grad()\n",
    "            optimizer_EBM.zero_grad()\n",
    "            \n",
    "            set_requires_grad(netEBM, False)\n",
    "            \n",
    "            # ============================================\n",
    "            #            Forward\n",
    "            # ============================================\n",
    "            with torch.no_grad():\n",
    "                Tfake = ???\n",
    "            Sfake = ???\n",
    "            \n",
    "            #            sampling energy-based model samples via Langevin Dynamics\n",
    "            Tfake_MCMC = ?????\n",
    "           \n",
    "        \n",
    "            # ============================================\n",
    "            #            TRAIN EBM\n",
    "            # ============================================\n",
    "            set_requires_grad(netEBM, True)\n",
    "            #            Energy-based Model Loss for netEBM\n",
    "            ebm_loss =  ?\n",
    "            ebm_loss.backward() \n",
    "            optimizer_EBM.step()\n",
    "            optimizer_EBM.zero_grad()\n",
    "                    \n",
    "            \n",
    "            # ============================================\n",
    "            #            TRAIN Generator\n",
    "            # ============================================\n",
    "           \n",
    "            set_requires_grad(netEBM, False)\n",
    "            set_requires_grad(netG_teacher_A, True)\n",
    "            \n",
    "            \n",
    "            student_vgg_features = VGGNet(Sfake)\n",
    "            teacher_vgg_features = VGGNet(Tfake)\n",
    "            \n",
    "            #            Style Loss for netG_student\n",
    "            style_loss = ???\n",
    "            \n",
    "            \n",
    "            #            Feature Reconstruction Loss for netG_student\n",
    "            student_vgg_feature = student_vgg_features[1]\n",
    "            teacher_vgg_feature = teacher_vgg_features[1]\n",
    "            feat_loss = ??\n",
    "            \n",
    "            #            Total Variation Loss for netG_student\n",
    "            total_variation_loss = ???\n",
    "            \n",
    "            #            Negative Mutual Information Loss for netG_student\n",
    "            negative_mi = ????\n",
    "            \n",
    "            \n",
    "            student_loss = lambda_style * style_loss + lambda_feat * feat_loss + \\\n",
    "            lambda_tv * total_variation_loss + lambda_mi * negative_mi\n",
    "            \n",
    "            student_loss.backward()\n",
    "            optimizer_G_student.step()\n",
    "            \n",
    "            if student_iter % print_every == 0:\n",
    "                print ('Student Loss')\n",
    "                print('Epoch [{:5d}/{:5d}] | EBM_loss: {:6.4f} | Student_loss: {:6.4f}'.format(\n",
    "epoch, student_n_epoch, ebm_loss.item(), student_loss.item()))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from util import save_network, get_scheduler, update_learning_rate\n",
    "\n",
    "MODEL_NAMES=['netG_teacher_A', 'netG_teacher_B', 'netG_student', 'netD_teacher_A', 'netD_teacher_B', 'netEBM']\n",
    "MODELS =[netG_teacher_A, netG_teacher_B, netG_student, netD_teacher_A, netD_teacher_B, netEBM]\n",
    "OPTIMIZERS=[optimizer_G_student, optimizer_G_teacher, optimizer_D_teacher, optimizer_EBM]\n",
    "SCHEDULERS = get_scheduler(OPTIMIZERS, 100, 100)\n",
    "def training_loop(dataloader, student_dataloader, n_epochs=200):\n",
    "    import time\n",
    "    since = time.time()\n",
    "    print_every=100\n",
    "    sample_every=100\n",
    "    save_latest_freq=25000\n",
    "    \n",
    "    losses = []      # keep track of losses over time\n",
    "\n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'                      \n",
    "    total_iter = 0\n",
    "    student_epoch_period = 10\n",
    "    student_epochs_per_period = 40\n",
    "    student_epoch = 1\n",
    "    student_n_epoch = (n_epochs//student_epoch_period) * student_epochs_per_period\n",
    "    VGGNet = VGGFeature().to(device)\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "\n",
    "        # Reset iterators for each epoch\n",
    "        #if epoch % batches_per_epoch == 0:\n",
    "            #iter_data = iter(dataloader)\n",
    "        for i, data_i in enumerate(dataloader):\n",
    "            total_iter += 1\n",
    "            # move images to device\n",
    "            data_i_A = data_i['A'].to(device) \n",
    "            data_i_B = data_i['B'].to(device) \n",
    "                \n",
    "                \n",
    "            \"\"\"       Teacher Training       \"\"\" \n",
    "            loss_G_teacher_A, loss_G_teacher_B, loss_D_teacher_A, loss_D_teacher_B = \\\n",
    "            optimize_teacher(data_i_A, data_i_B)\n",
    "        \n",
    "            # Print\n",
    "            if total_iter % print_every == 0:\n",
    "                time_elapsed = time.time() - since\n",
    "                print ('Teacher Loss')\n",
    "                print('Epoch [{:5d}/{:5d}] | d_A_loss: {:6.4f} | d_B_loss: {:6.4f} | g_A_loss: {:6.4f} | g_B_loss: {:6.4f} time : {:.0f}m {:.0f}s'.format(\n",
    "                    epoch, n_epochs, loss_D_teacher_A.item(), loss_D_teacher_B.item(),loss_G_teacher_A.item(), loss_G_teacher_B.item() ,time_elapsed // 60, time_elapsed % 60))\n",
    "        \n",
    "            if total_iter % save_latest_freq == 0:\n",
    "                save_network(MODELS, MODEL_NAMES, 'iter_%d'%total_iter, './checkpoint')       \n",
    "            \n",
    "            \"\"\"\n",
    "            checkpoint_every=1\n",
    "            if epoch % checkpoint_every == 0:\n",
    "                checkpoint(epoch, G_XtoY, G_YtoX, D_X, D_Y)\n",
    "            \"\"\"\n",
    "            \n",
    "        \n",
    "        if epoch % student_epoch_period == 0: \n",
    "            optimize_student_ebm(VGGNet, student_dataloader, student_n_epoch, student_epoch, student_epochs_per_period)\n",
    "            student_epoch += student_epochs_per_period\n",
    "        \n",
    "        \n",
    "        update_learning_rate(SCHEDULERS, OPTIMIZERS)\n",
    "            \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epochs=200\n",
    "dataloader = create_dataloader(dataroot='./database/horse2zebra', shuffle=True, batch_size=1, num_workers=4)\n",
    "student_dataloader = create_dataloader(dataroot='./database/horse2zebra', shuffle=True, batch_size=1, num_workers=4)\n",
    "training_loop(dataloader, student_dataloader, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "samsung",
   "language": "python",
   "name": "samsung"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
